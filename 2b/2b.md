# Расчётное задание 2b "Аппроксимация результатов измерений зависимых переменных"
- студент: Сыров Егор Романович
- группа: 5130901/30201


```python
import numpy as np
import pandas as pd
import scipy
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge
import statsmodels.api as sm
from sklearn.metrics import r2_score, mean_squared_error
from scipy.interpolate import interp1d, CubicSpline, PchipInterpolator
from scipy.optimize import curve_fit
import warnings

warnings.filterwarnings("ignore")


# Считывание данных из файла
with open("Task_2b.txt", "r") as file:
    nx = int(file.readline().split("=")[1])
    ny = int(file.readline().split("=")[1])
    x = list(map(np.float64, file.readline().split("=", 1)[1].split()))
    y = []
    for line in file:
        y_i = list(map(np.float64, line.split("=", 1)[1].split()))
        y.append(y_i)

# Приводим к np.array для параллельных вычислений
x = np.array(x)
y = np.array(y)
df = pd.DataFrame(y, index=x, columns=[f"y_{i+1}" for i in range(ny)])
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y_1</th>
      <th>y_2</th>
      <th>y_3</th>
      <th>y_4</th>
      <th>y_5</th>
      <th>y_6</th>
      <th>y_7</th>
      <th>y_8</th>
      <th>y_9</th>
      <th>y_10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>-2.0</th>
      <td>-16.25280</td>
      <td>-12.49660</td>
      <td>7.29730</td>
      <td>-1.044000</td>
      <td>-23.13510</td>
      <td>-4.91375</td>
      <td>-38.86790</td>
      <td>-1.71512</td>
      <td>-29.99650</td>
      <td>-8.76051</td>
    </tr>
    <tr>
      <th>-1.9</th>
      <td>-20.55480</td>
      <td>-25.00250</td>
      <td>-49.13580</td>
      <td>-35.911300</td>
      <td>-1.44807</td>
      <td>-2.95197</td>
      <td>-50.14930</td>
      <td>-16.44190</td>
      <td>-22.28700</td>
      <td>-39.13680</td>
    </tr>
    <tr>
      <th>-1.8</th>
      <td>-58.14500</td>
      <td>-31.38800</td>
      <td>-47.13900</td>
      <td>-52.170900</td>
      <td>-36.60580</td>
      <td>-60.95190</td>
      <td>-48.92500</td>
      <td>-49.29130</td>
      <td>-24.92270</td>
      <td>-41.46350</td>
    </tr>
    <tr>
      <th>-1.7</th>
      <td>-61.17980</td>
      <td>-67.13150</td>
      <td>-94.42240</td>
      <td>-47.787500</td>
      <td>-67.94720</td>
      <td>-74.90080</td>
      <td>-74.55220</td>
      <td>-78.82430</td>
      <td>-57.32470</td>
      <td>-84.94240</td>
    </tr>
    <tr>
      <th>-1.6</th>
      <td>-59.11520</td>
      <td>-107.44000</td>
      <td>-55.41440</td>
      <td>-106.930000</td>
      <td>-48.29990</td>
      <td>-60.90860</td>
      <td>-82.65450</td>
      <td>-80.99280</td>
      <td>-90.16550</td>
      <td>-68.63910</td>
    </tr>
    <tr>
      <th>-1.5</th>
      <td>-78.41000</td>
      <td>-88.77780</td>
      <td>-60.87250</td>
      <td>-64.463400</td>
      <td>-63.67370</td>
      <td>-75.20020</td>
      <td>-62.05220</td>
      <td>-77.42470</td>
      <td>-61.74030</td>
      <td>-41.70070</td>
    </tr>
    <tr>
      <th>-1.4</th>
      <td>-69.56390</td>
      <td>-51.53460</td>
      <td>-97.38320</td>
      <td>-62.444000</td>
      <td>-69.00500</td>
      <td>-68.26720</td>
      <td>-67.38880</td>
      <td>-80.12200</td>
      <td>-67.74050</td>
      <td>-66.75540</td>
    </tr>
    <tr>
      <th>-1.3</th>
      <td>-20.52090</td>
      <td>-39.72000</td>
      <td>-58.42640</td>
      <td>-43.412200</td>
      <td>-45.59140</td>
      <td>-27.14040</td>
      <td>-62.03190</td>
      <td>-37.82690</td>
      <td>-40.32290</td>
      <td>-48.15950</td>
    </tr>
    <tr>
      <th>-1.2</th>
      <td>-24.31430</td>
      <td>-33.43480</td>
      <td>-10.88460</td>
      <td>-37.963800</td>
      <td>-13.34650</td>
      <td>-15.08710</td>
      <td>1.75719</td>
      <td>-18.26120</td>
      <td>-33.91410</td>
      <td>-24.74440</td>
    </tr>
    <tr>
      <th>-1.1</th>
      <td>-36.89140</td>
      <td>-2.36692</td>
      <td>-8.26458</td>
      <td>-14.640700</td>
      <td>-9.20905</td>
      <td>-10.93300</td>
      <td>-22.00890</td>
      <td>5.77602</td>
      <td>-35.03000</td>
      <td>2.10727</td>
    </tr>
    <tr>
      <th>-1.0</th>
      <td>25.16250</td>
      <td>1.26959</td>
      <td>26.66980</td>
      <td>14.766100</td>
      <td>8.22589</td>
      <td>26.23410</td>
      <td>14.37080</td>
      <td>13.73400</td>
      <td>14.81640</td>
      <td>9.64626</td>
    </tr>
    <tr>
      <th>-0.9</th>
      <td>24.20520</td>
      <td>24.32660</td>
      <td>28.21330</td>
      <td>42.530300</td>
      <td>40.26890</td>
      <td>32.56770</td>
      <td>23.43480</td>
      <td>23.24390</td>
      <td>38.47200</td>
      <td>30.11650</td>
    </tr>
    <tr>
      <th>-0.8</th>
      <td>50.49530</td>
      <td>54.84380</td>
      <td>54.84760</td>
      <td>68.099200</td>
      <td>54.78800</td>
      <td>45.03750</td>
      <td>65.81160</td>
      <td>66.23170</td>
      <td>52.05540</td>
      <td>75.54300</td>
    </tr>
    <tr>
      <th>-0.7</th>
      <td>88.39780</td>
      <td>71.61780</td>
      <td>71.52090</td>
      <td>69.603000</td>
      <td>80.55080</td>
      <td>79.53730</td>
      <td>74.84780</td>
      <td>78.39850</td>
      <td>71.38160</td>
      <td>78.44810</td>
    </tr>
    <tr>
      <th>-0.6</th>
      <td>84.64640</td>
      <td>82.08690</td>
      <td>91.61280</td>
      <td>86.504100</td>
      <td>83.29460</td>
      <td>90.60820</td>
      <td>88.16110</td>
      <td>82.17800</td>
      <td>93.69620</td>
      <td>77.61550</td>
    </tr>
    <tr>
      <th>-0.5</th>
      <td>82.86810</td>
      <td>87.33740</td>
      <td>89.58200</td>
      <td>94.315800</td>
      <td>92.14700</td>
      <td>94.06900</td>
      <td>89.96600</td>
      <td>88.08700</td>
      <td>84.68110</td>
      <td>88.73820</td>
    </tr>
    <tr>
      <th>-0.4</th>
      <td>85.85680</td>
      <td>85.07860</td>
      <td>81.92990</td>
      <td>84.820000</td>
      <td>84.48320</td>
      <td>84.62680</td>
      <td>83.47090</td>
      <td>85.80940</td>
      <td>88.19820</td>
      <td>84.42860</td>
    </tr>
    <tr>
      <th>-0.3</th>
      <td>72.20100</td>
      <td>72.18490</td>
      <td>72.20720</td>
      <td>72.193700</td>
      <td>72.19700</td>
      <td>72.20380</td>
      <td>72.20630</td>
      <td>72.19980</td>
      <td>72.20430</td>
      <td>72.19410</td>
    </tr>
    <tr>
      <th>-0.2</th>
      <td>53.27660</td>
      <td>49.20360</td>
      <td>52.86650</td>
      <td>52.339800</td>
      <td>53.17980</td>
      <td>52.39320</td>
      <td>57.13990</td>
      <td>54.82550</td>
      <td>49.83570</td>
      <td>51.25840</td>
    </tr>
    <tr>
      <th>-0.1</th>
      <td>30.81130</td>
      <td>30.62290</td>
      <td>28.98860</td>
      <td>27.055000</td>
      <td>33.69960</td>
      <td>30.18320</td>
      <td>26.18480</td>
      <td>21.71400</td>
      <td>28.83670</td>
      <td>27.79900</td>
    </tr>
    <tr>
      <th>0.0</th>
      <td>1.87762</td>
      <td>6.36974</td>
      <td>7.67827</td>
      <td>4.231120</td>
      <td>-3.17460</td>
      <td>-5.21817</td>
      <td>7.49572</td>
      <td>-2.60832</td>
      <td>2.33135</td>
      <td>7.58816</td>
    </tr>
    <tr>
      <th>0.1</th>
      <td>-16.56850</td>
      <td>-24.69960</td>
      <td>-29.71090</td>
      <td>-20.104200</td>
      <td>-32.30700</td>
      <td>-35.43580</td>
      <td>-28.36010</td>
      <td>-36.32010</td>
      <td>-35.75430</td>
      <td>-32.55800</td>
    </tr>
    <tr>
      <th>0.2</th>
      <td>-40.11050</td>
      <td>-56.47820</td>
      <td>-45.81570</td>
      <td>-31.801500</td>
      <td>-53.13440</td>
      <td>-59.80900</td>
      <td>-37.74350</td>
      <td>-50.29940</td>
      <td>-45.03420</td>
      <td>-52.93660</td>
    </tr>
    <tr>
      <th>0.3</th>
      <td>-84.41320</td>
      <td>-70.81300</td>
      <td>-63.24310</td>
      <td>-82.369100</td>
      <td>-79.25920</td>
      <td>-84.45180</td>
      <td>-66.42610</td>
      <td>-61.18970</td>
      <td>-76.50900</td>
      <td>-89.86840</td>
    </tr>
    <tr>
      <th>0.4</th>
      <td>-77.34230</td>
      <td>-89.18570</td>
      <td>-84.59460</td>
      <td>-88.005000</td>
      <td>-97.55160</td>
      <td>-107.14000</td>
      <td>-54.74830</td>
      <td>-69.65960</td>
      <td>-82.57200</td>
      <td>-86.25270</td>
    </tr>
    <tr>
      <th>0.5</th>
      <td>-99.74290</td>
      <td>-76.78290</td>
      <td>-102.41000</td>
      <td>-100.330000</td>
      <td>-120.92000</td>
      <td>-100.53800</td>
      <td>-97.65220</td>
      <td>-93.16000</td>
      <td>-108.61500</td>
      <td>-103.56200</td>
    </tr>
    <tr>
      <th>0.6</th>
      <td>-84.22320</td>
      <td>-89.61860</td>
      <td>-125.14300</td>
      <td>-100.877000</td>
      <td>-89.10980</td>
      <td>-77.37390</td>
      <td>-117.32800</td>
      <td>-108.62900</td>
      <td>-104.62500</td>
      <td>-93.26660</td>
    </tr>
    <tr>
      <th>0.7</th>
      <td>-89.11610</td>
      <td>-89.23500</td>
      <td>-94.40880</td>
      <td>-61.092000</td>
      <td>-78.93390</td>
      <td>-72.55150</td>
      <td>-77.95850</td>
      <td>-77.14040</td>
      <td>-88.32920</td>
      <td>-93.71710</td>
    </tr>
    <tr>
      <th>0.8</th>
      <td>-73.37740</td>
      <td>-70.96020</td>
      <td>-80.79430</td>
      <td>-80.334000</td>
      <td>-84.32890</td>
      <td>-54.67490</td>
      <td>-61.25170</td>
      <td>-48.60700</td>
      <td>-75.17120</td>
      <td>-81.72900</td>
    </tr>
    <tr>
      <th>0.9</th>
      <td>-49.36430</td>
      <td>-22.05880</td>
      <td>-34.29220</td>
      <td>-56.543100</td>
      <td>-52.88430</td>
      <td>-30.04790</td>
      <td>-19.95110</td>
      <td>-47.23030</td>
      <td>-21.56150</td>
      <td>-72.99580</td>
    </tr>
    <tr>
      <th>1.0</th>
      <td>-26.85610</td>
      <td>-29.15310</td>
      <td>-14.65020</td>
      <td>-15.445200</td>
      <td>-0.33315</td>
      <td>-7.13160</td>
      <td>-12.53740</td>
      <td>-33.59610</td>
      <td>-59.96020</td>
      <td>-7.30777</td>
    </tr>
    <tr>
      <th>1.1</th>
      <td>10.39230</td>
      <td>26.19110</td>
      <td>8.90374</td>
      <td>0.116707</td>
      <td>35.56090</td>
      <td>29.32620</td>
      <td>7.95788</td>
      <td>-11.31850</td>
      <td>2.20500</td>
      <td>2.84689</td>
    </tr>
    <tr>
      <th>1.2</th>
      <td>49.09790</td>
      <td>51.73950</td>
      <td>48.98390</td>
      <td>37.108000</td>
      <td>29.43710</td>
      <td>2.07751</td>
      <td>28.40340</td>
      <td>45.55680</td>
      <td>76.28530</td>
      <td>45.41760</td>
    </tr>
    <tr>
      <th>1.3</th>
      <td>4.71816</td>
      <td>67.88490</td>
      <td>56.43550</td>
      <td>45.567600</td>
      <td>53.32400</td>
      <td>61.09340</td>
      <td>39.73370</td>
      <td>42.13530</td>
      <td>79.19430</td>
      <td>63.05340</td>
    </tr>
    <tr>
      <th>1.4</th>
      <td>83.03530</td>
      <td>66.93950</td>
      <td>58.90850</td>
      <td>45.360000</td>
      <td>82.51590</td>
      <td>42.54700</td>
      <td>89.63680</td>
      <td>74.44700</td>
      <td>68.11180</td>
      <td>63.30610</td>
    </tr>
    <tr>
      <th>1.5</th>
      <td>60.72060</td>
      <td>98.87040</td>
      <td>66.38410</td>
      <td>89.421100</td>
      <td>74.63350</td>
      <td>83.54070</td>
      <td>71.53060</td>
      <td>75.41250</td>
      <td>56.53790</td>
      <td>91.45110</td>
    </tr>
    <tr>
      <th>1.6</th>
      <td>71.70330</td>
      <td>66.53750</td>
      <td>53.09620</td>
      <td>96.545400</td>
      <td>95.34780</td>
      <td>74.45760</td>
      <td>63.14530</td>
      <td>30.88590</td>
      <td>76.69290</td>
      <td>88.99850</td>
    </tr>
    <tr>
      <th>1.7</th>
      <td>71.49980</td>
      <td>43.35520</td>
      <td>65.60150</td>
      <td>55.577400</td>
      <td>47.78450</td>
      <td>46.50350</td>
      <td>72.19090</td>
      <td>69.44980</td>
      <td>38.63010</td>
      <td>84.72370</td>
    </tr>
    <tr>
      <th>1.8</th>
      <td>59.97610</td>
      <td>88.45850</td>
      <td>32.80270</td>
      <td>63.053800</td>
      <td>53.58450</td>
      <td>52.63180</td>
      <td>48.12140</td>
      <td>83.66120</td>
      <td>35.35160</td>
      <td>67.02210</td>
    </tr>
    <tr>
      <th>1.9</th>
      <td>30.55780</td>
      <td>36.55290</td>
      <td>34.13320</td>
      <td>37.357800</td>
      <td>29.00060</td>
      <td>37.83890</td>
      <td>33.67530</td>
      <td>49.18110</td>
      <td>33.02480</td>
      <td>40.20160</td>
    </tr>
    <tr>
      <th>2.0</th>
      <td>21.27600</td>
      <td>18.47810</td>
      <td>6.78533</td>
      <td>18.360500</td>
      <td>49.25980</td>
      <td>18.05480</td>
      <td>2.22091</td>
      <td>48.53600</td>
      <td>18.05710</td>
      <td>15.47230</td>
    </tr>
  </tbody>
</table>
</div>



# Вычислить в каждой точке средние арифметические значения, оценки дисперсий, параметрические толерантные пределы для погрешностей, доверительные интервалы для математических ожиданий, проверить гипотезу о равенстве дисперсий в этих точках по критерию Кочрена (см. приложение 3)


```python
means = np.mean(y, axis=1)
variances = np.var(y, axis=1, ddof=1)
std_devs = np.sqrt(variances)
# Относительное стандартное отклонение (коэффициенты вариации)
cv = std_devs / np.abs(means) * 100


# Доверительные интервалы для Математического Ожидания
confidence_level = 0.95
alpha = 1 - confidence_level

ci_lower = np.zeros(nx)
ci_upper = np.zeros(nx)

for i in range(nx):
    ci = stats.t.interval(
        confidence_level, df=ny - 1, loc=means[i], scale=std_devs[i] / np.sqrt(ny)
    )
    ci_lower[i], ci_upper[i] = ci

# Вычисление толерантных пределов
beta = 0.80  # Доля генеральной совокупности
gamma = 0.95  # Доверительная вероятность

# Вычисление толерантного множителя K для нормального распределения
# Используем аппроксимацию Хау (Howe) для больших n
# K = z_{(1+beta)/2} * sqrt((n-1) * (1 + 1/n) / chi2_{1-gamma, n-1})

z_beta = stats.norm.ppf((1 + beta) / 2)
chi2_gamma = stats.chi2.ppf(1 - gamma, df=ny - 1)

K = z_beta * np.sqrt((ny - 1) * (1 + 1 / ny) / chi2_gamma)

tolerance_lower = means - K * std_devs
tolerance_upper = means + K * std_devs

print(f"Толерантный множитель K = {K:.4f}")
print(f"Для первой точки (x={x[0]:.4f}):")
print(f"\tСреднее = {means[0]:.4f}")
print(f"\tСтандартное отклонение = {std_devs[0]:.4f}")
print(f"Толерантные пределы: [{tolerance_lower[0]:.4f}, {tolerance_upper[0]:.4f}]")

# Статистика Кочрена: G = max(s_i^2) / sum(s_i^2)
G = np.max(variances) / np.sum(variances)


# Критическое значение для критерия Кочрена
def cochran_critical(n_groups, n_obs, alpha=0.05):
    """
    Аппроксимация критического значения критерия Кочрена
    n_groups - количество групп (nx)
    n_obs - количество наблюдений в каждой группе (ny)
    """
    df = n_obs - 1
    F_crit = stats.f.ppf(1 - alpha, dfn=df, dfd=df * (n_groups - 1))
    return F_crit / (F_crit + n_groups - 1)


G_crit = cochran_critical(nx, ny, alpha=0.05)

print(f"\nКритерий Кочрена:")
print(f"  Статистика G = {G:.6f}")
print(f"  Критическое значение G_crit = {G_crit:.6f}")
print(
    f"  Гипотеза о равенстве дисперсий {'принимается' if G < G_crit else 'отвергается'} на уровне значимости 0.05"
)
```

    Толерантный множитель K = 2.2113
    Для первой точки (x=-2.0000):
    	Среднее = -12.9885
    	Стандартное отклонение = 14.2876
    Толерантные пределы: [-44.5829, 18.6059]
    
    Критерий Кочрена:
      Статистика G = 0.065226
      Критическое значение G_crit = 0.045481
      Гипотеза о равенстве дисперсий отвергается на уровне значимости 0.05


# Визуализация математичекого ожидания


```python
# Визуализация результатов
plt.figure(figsize=(14, 8))

# Средние значения
plt.plot(x, means, "bo-", linewidth=2, markersize=8, label="Средние значения")

# Доверительные интервалы для математических ожиданий
plt.fill_between(
    x,
    ci_lower,
    ci_upper,
    alpha=0.3,
    color="blue",
    label=f"95% доверительный интервал (n={ny})",
)

# Параметрические толерантные пределы
plt.fill_between(
    x,
    tolerance_lower,
    tolerance_upper,
    alpha=0.2,
    color="red",
    label=f"80/95% толерантные пределы (K={K:.2f})",
)

plt.grid()
plt.xlabel("x", fontsize=12)
plt.ylabel("y", fontsize=12)
plt.title("Средние значения с доверительными и толерантными пределами", fontsize=14)
plt.legend(fontsize=10)
plt.axhline(y=0, color="k", linestyle="--", alpha=0.3)
```




    <matplotlib.lines.Line2D at 0x7fd5b41b7c50>




    
![png](2b_files/2b_5_1.png)
    


# Визуализация дисперсий и результат критерия Кочрена


```python
plt.figure(figsize=(14, 6))

plt.plot(x, variances, "ro-", linewidth=2, markersize=8, label="Дисперсия")
max_var_idx = np.argmax(variances)
plt.axhline(
    y=np.max(variances),
    color="r",
    linestyle="--",
    alpha=0.7,
    label=f"Максимальная дисперсия (x={x[max_var_idx]})",
)

plt.grid(True, alpha=0.3)
plt.xlabel("x", fontsize=12)
plt.ylabel("Дисперсия", fontsize=12)
plt.title("Дисперсия измерений в каждой точке", fontsize=14)
plt.legend(fontsize=10)

plt.text(
    0.05,
    0.80,
    f"Критерий Кочрена: G = {G:.4f}, G_crit = {G_crit:.4f}\n"
    f"Гипотеза о равенстве дисперсий "
    f'{"принимается" if G < G_crit else "отвергается"}',
    transform=plt.gca().transAxes,
    fontsize=12,
    bbox=dict(facecolor="white", alpha=0.7),
)
```




    Text(0.05, 0.8, 'Критерий Кочрена: G = 0.0652, G_crit = 0.0455\nГипотеза о равенстве дисперсий отвергается')




    
![png](2b_files/2b_7_1.png)
    


# Тепловая карта корреляций между точками


```python
correlation_matrix = np.corrcoef(y)

print(correlation_matrix)

plt.figure(figsize=(14, 12))
sns.heatmap(
    correlation_matrix,
    xticklabels=np.round(x, 2),
    yticklabels=np.round(x, 2),
    cmap="coolwarm",
    center=0,
    annot=False,
    square=True,
)
```

    [[ 1.         -0.02442697 -0.38356399 ...  0.22511334  0.48978497
       0.09546305]
     [-0.02442697  1.         -0.00981815 ...  0.22156875 -0.01413118
       0.73317554]
     [-0.38356399 -0.00981815  1.         ... -0.02060997 -0.18120288
       0.10785253]
     ...
     [ 0.22511334  0.22156875 -0.02060997 ...  1.          0.58429506
       0.41851193]
     [ 0.48978497 -0.01413118 -0.18120288 ...  0.58429506  1.
       0.26090394]
     [ 0.09546305  0.73317554  0.10785253 ...  0.41851193  0.26090394
       1.        ]]





    <Axes: >




    
![png](2b_files/2b_9_2.png)
    


# Анализ тепловой карты корреляции
Полученный график коррелированности говорит о том, что данные слабо коррилированны - линейная связь отсутствует

# Произвести последовательную полиномиальную аппроксимацию Прим. В качестве значений y при аппроксимации необходимо использовать средние арифметические значения.


```python
use_weighted = G <= G_crit


def polynomial_approximation(x, y, variances, degree, use_weighted):
    """
    Возвращает коэффициенты полинома для заданной степени.

    Аргументы:
        x: координаты точек
        y: средние значения y
        variances: дисперсии в каждой точке
        degree: степень полинома
        use_weighted: использовать взвешенный МНК (если дисперсии неравные)
    """
    X = np.vander(x, degree + 1)
    if use_weighted:
        weights = 1 / variances
        W = np.diag(weights)
        coeffs = np.linalg.solve(X.T @ W @ X, X.T @ W @ y)
    else:
        coeffs = np.linalg.solve(X.T @ X, X.T @ y)
    return coeffs


coeffs_0 = polynomial_approximation(x, means, variances, 0, use_weighted)
y_poly_0 = np.full_like(x, coeffs_0[0])

print(f"Коэффициент нулевой степени (константа): {coeffs_0[0]:.4f}")

# Первая степень (линейная модель: y = ax + b)
coeffs_1 = polynomial_approximation(x, means, variances, 1, use_weighted)
y_poly_1 = np.polyval(coeffs_1, x)

print(f"Коэффициенты линейной модели: a = {coeffs_1[0]:.4f}, b = {coeffs_1[1]:.4f}")

plt.figure(figsize=(10, 6))
plt.scatter(x, means, color="blue", label="Средние значения $\\overline{y}_i$")
plt.plot(
    x,
    y_poly_1,
    "r-",
    linewidth=2,
    label=f"Полином 1-ой степени (a={coeffs_1[0]:.2f}, b={coeffs_1[1]:.2f})",
)
plt.fill_between(
    x,
    y_poly_1,
    -2 * np.sqrt(variances),
    y_poly_1 + 2 * np.sqrt(variances),
    alpha=0.2,
    color="red",
    label="Доверительная область (2$\\sigma$)",
)
plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Линейная аппроксимация")
plt.legend()


# Подбор оптимальной степени полинома
def residual_variances(x, y, coeffs, degree):
    y_pred = np.polyval(coeffs, x)
    residuals = y - y_pred
    return np.var(residuals, ddof=degree + 1)


degrees = np.arange(6)
residual_vars = []

for d in degrees:
    coeffs = polynomial_approximation(x, means, variances, d, use_weighted)
    res_var = residual_variances(x, means, coeffs, d)
    residual_vars.append(res_var)
    print(f"Степень {d}: остаточная дисперсия = {res_var:.4f}")

plt.figure(figsize=(8, 5))
plt.plot(degrees, residual_vars, "o-", markersize=8)
plt.xlabel("Степень полинома")
plt.ylabel("Остаточная дисперсия")
plt.title("Оценка качества аппроксимации")
plt.grid(True, alpha=0.3)
```

    Коэффициент нулевой степени (константа): -0.0506
    Коэффициенты линейной модели: a = 14.0425, b = -0.0506
    Степень 0: остаточная дисперсия = 3646.5968
    Степень 1: остаточная дисперсия = 3449.8738
    Степень 2: остаточная дисперсия = 3536.5973
    Степень 3: остаточная дисперсия = 2810.8266
    Степень 4: остаточная дисперсия = 2888.8045
    Степень 5: остаточная дисперсия = 455.8899



    
![png](2b_files/2b_12_1.png)
    



    
![png](2b_files/2b_12_2.png)
    


После проведения оценки качества аппроксимации можно сделать вывод о том, чтоюы описать синусоиду необходим полином гораздо большей степени (>> 6).

# Подбор оптимальной степени полинома с помощью критерия Фишера


```python
def calculate_f_statistic(x, y, q, nx, ny):
    """
    Вычисляет F-статистику для проверки гипотезы о степени полинома q.

    Аргументы:
        x: координаты точек (x_vals)
        y: средние значения (y_mean)
        q: текущая степень полинома
        nx: количество точек (41)
        ny: количество измерений в каждой точке (10)

    Возвращает:
        F: F-статистика
        F_crit: критическое значение F-распределения
        R2: коэффициент детерминации
    """

    # Построение полиномиальной модели
    coeffs = np.polyfit(x, y, q)
    y_pred = np.polyval(coeffs, x)

    # Общая сумма квадратов (SST)
    y_mean_global = np.mean(y)
    SST = np.sum((y - y_mean_global) ** 2)

    # Остаточная сумма квадратов (SSE)
    SSE = np.sum((y - y_pred) ** 2)

    # Коэффициент детерминаци
    R2 = 1 - SSE / SST

    # Выбор формулы для F-статичтики
    if ny > nx - q - 1:
        numerator = ny - nx + q + 1
        denominator = (ny - q - q) * (ny - 1)
        F = (numerator / denominator) * R2
    else:
        F = R2 / (nx - q - 1)

    # Критическое значение F-распределения
    # df1 = 1 (увеличение степени на 1), df2 = nx * ny - (q + 1)
    df1 = 1
    df2 = nx * ny - (q + 1)
    F_crit = stats.f.ppf(0.95, df1, df2)

    return F, F_crit, R2


def find_optimal_polinomial_degree(x, y, nx, ny, max_degree=10):
    """
    Находит оптимальную степень полинома, проверяя гипотезу для q = 0, 1, ..., max_degree.

    Возвращает:
        optimal_q: оптимальная степень
        F_values: список F-статистик для каждой степени
    """
    q = 0
    F_values = []
    optimal_q = None

    while q <= max_degree:
        F, F_crit, R2 = calculate_f_statistic(x, y, q, nx, ny)
        F_values.append(F)

        print(f"Степень q = {q}: F= {F:.4f}, F_crit = {F_crit:.4f}, R2 = {R2:.4f}")

        # Если гипотеза НЕ отвергается (F <= F_crit) - останавливаемся
        if F <= F_crit:
            optimal_q = q
            print(
                f"Гипотеза о степени q = {q} НЕ отвергается. Оптимальная степень: {q}"
            )
            break
        else:
            print(f"Гипотеза о степени q = {q} отвергается. Увеличиваем степень.")
            q += 1

    if optimal_q is None:
        optimal_q = max_degree
        print(
            f"Достигнута максимальная степень {max_degree}. Оптимальная степень: {optimal_q}"
        )

    return optimal_q, F_values


optimal_q, F_values = find_optimal_polinomial_degree(x, means, nx, ny, max_degree=10)

# Построение оптимальной модели
coeffs_opt = np.polyfit(x, means, optimal_q)
y_opt = np.polyval(coeffs_opt, x)

plt.figure(figsize=(10, 8))
plt.scatter(x, means, color="blue", label="Средние занчения $\\overline{y}_i$")
plt.plot(x, y_opt, "r-", linewidth=2, label=f"Полином степени {optimal_q}")
plt.fill_between(
    x,
    y_opt - 2 * np.sqrt(variances),
    y_opt + 2 * np.sqrt(variances),
    alpha=0.2,
    color="red",
    label="Доверительная область ($2\\sigma$)",
)
plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title(f"Полиномиальная аппроксимация (степень {optimal_q})")
plt.legend()
```

    Степень q = 0: F= 0.0000, F_crit = 3.8643, R2 = 0.0000
    Гипотеза о степени q = 0 НЕ отвергается. Оптимальная степень: 0





    <matplotlib.legend.Legend at 0x7fd5b354c7d0>




    
![png](2b_files/2b_15_2.png)
    


Полиномиальная аппроксимация не может должным образом (см. Критерий Фишера) обесепчить аппроксимацию. Так как повышение степени полинома не добавляет результату аппроксимации точности - алгоритм поиска оптимальной степени полинома останавливается на нулевой степени. Для описания синусоиды нудно использовать более сложные способы аппроксимации (например Сплайны, или среднеквадратическую аппроксимацию).

# Вычислить корреляционную матрицу и коэффициенты корреляции между оценками коэффициентов по матрице ковариации


```python
def calculate_covariance_matrix(x, y, q, use_weighted=False, weights=None):
    X = np.vander(x, q + 1)

    if use_weighted and weights is not None:
        W = np.diag(weights)
        XTWX = X.T @ W @ X
        sigma2 = 1.0
        Sa = np.linalg.inv(XTWX) * sigma2
    else:
        XTX = X.T @ X
        y_pred = np.polyval(np.polyfit(x, y, q), x)
        residuals = y - y_pred
        sigma2 = np.sum(residuals**2) / (len(x) - q - 1)
        Sa = np.linalg.inv(XTX) * sigma2

    return Sa


Sa = calculate_covariance_matrix(x, means, optimal_q)
print(f"Ковариационная матрица S_a (размер {optimal_q+1}x{optimal_q+1}):\n", Sa)
```

    Ковариационная матрица S_a (размер 1x1):
     [[88.94138551]]


# Пусть была получена степень q полинома, прошедшая гипотезу о степени полинома. Произвести все те же действия для полинома степени, равной k-1 (вычислить коэффициенты и корреляцию между ними). Сравнить результаты для степени q и k-1 (качество аппроксимации, корреляционная матрица коэффициентов, матрица ковариации исходных данных и ее обусловленность).


```python
k = nx - 1
coeffs_k1 = np.polyfit(x, means, k)
y_k1 = np.polyval(coeffs_k1, x)
residuals_k1 = means - y_k1

# Для полинома степени q
residuals_q = means - y_opt  # где y_opt - значения полинома степени q
residual_var_q = np.sum(residuals_q**2) / (k - optimal_q - 1)

# Для полинома степени k-1
residual_var_k1 = np.sum(residuals_k1**2) / (
    k - (k - 1) - 1
)  # Деление на 0 - особый случай

# В случае k-1 степени полином точно проходит через все точки
# Поэтому остаточная дисперсия должна быть близка к нулю
print(f"Остаточная дисперсия для степени q = {optimal_q}: {residual_var_q:.4f}")
print(
    f"Остаточная дисперсия для степени k-1: {np.sum(residuals_k1**2):.4e} (теоретически 0)"
)

plt.figure(figsize=(12, 8))
plt.scatter(x, means, color="blue", label="Средние значения $\\overline{y}_i$")

# Полином степени q
plt.plot(x, y_opt, "r-", linewidth=2, label=f"Полином степени {optimal_q}")

# Полином степени k-1
plt.plot(x, y_k1, "g--", linewidth=1.5, label=f"Полином степени {k-1}")

plt.fill_between(
    x,
    y_opt - 2 * np.sqrt(variances),
    y_opt + 2 * np.sqrt(variances),
    alpha=0.2,
    color="red",
    label="Доверительная область (2σ) для q",
)

plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Сравнение полиномиальных аппроксимаций")
plt.legend()


# Корреляционная матрица
# Для полинома степени q
cov_q = calculate_covariance_matrix(x, means, optimal_q)
cond_q = np.linalg.cond(cov_q)

# Для полинома степени k-1
cov_k1 = calculate_covariance_matrix(x, means, k - 1)
cond_k1 = np.linalg.cond(cov_k1)

print(f"Число обусловленности для степени q = {optimal_q}: {cond_q:.2e}")
print(f"Число обусловленности для степени k-1: {cond_k1:.2e}")


# Коррреляционная матрица коэффициентов
def get_correlation_matrix(cov_matrix):
    stds = np.sqrt(np.diag(cov_matrix))
    corr = cov_matrix / np.outer(stds, stds)
    return corr


# Корреляционные матрицы
corr_q = get_correlation_matrix(cov_q)
corr_k1 = get_correlation_matrix(cov_k1)

# Визуализация корреляционных матриц
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.imshow(corr_q, cmap="coolwarm", vmin=-1, vmax=1)
plt.colorbar()
plt.title(f"Корреляционная матрица (степень {optimal_q})")
plt.xticks(range(optimal_q + 1), [f"a{i}" for i in range(optimal_q + 1)])
plt.yticks(range(optimal_q + 1), [f"a{i}" for i in range(optimal_q + 1)])

plt.subplot(1, 2, 2)
plt.imshow(corr_k1, cmap="coolwarm", vmin=-1, vmax=1)
plt.colorbar()
plt.title(f"Корреляционная матрица (степень {k-1})")
plt.xticks(range(k), [f"a{i}" for i in range(k)])
plt.yticks(range(k), [f"a{i}" for i in range(k)])

plt.tight_layout()
```

    Остаточная дисперсия для степени q = 0: 3740.0993
    Остаточная дисперсия для степени k-1: 8.8667e+01 (теоретически 0)
    Число обусловленности для степени q = 0: 1.00e+00
    Число обусловленности для степени k-1: 2.35e+19



    
![png](2b_files/2b_20_1.png)
    



    
![png](2b_files/2b_20_2.png)
    


# Аппроксимация другими способами

## Линейная регрессия


```python
X = x.reshape(-1, 1)
model_linear = LinearRegression()
model_linear.fit(X, means)
y_pred_linear = model_linear.predict(X)

r2_linear = r2_score(means, y_pred_linear)
mse_linear = mean_squared_error(means, y_pred_linear)
coeffs_linear = model_linear.coef_[0], model_linear.intercept_

print(f"Linear Regression: R² = {r2_linear:.4f}, MSE = {mse_linear:.4f}")
print(f"Коэффициенты: a = {coeffs_linear[0]:.4f}, b = {coeffs_linear[1]:.4f}")
```

    Linear Regression: R² = 0.0776, MSE = 3281.5872
    Коэффициенты: a = 14.0425, b = -0.0506


## Robust Regression (Робастная регрессия)


```python
X_sm = sm.add_constant(x)
model_robust = sm.RLM(means, X_sm, M=sm.robust.norms.HuberT())
results_robust = model_robust.fit()
y_pred_robust = results_robust.predict(X_sm)

# Метрики
r2_robust = r2_score(means, y_pred_robust)
mse_robust = mean_squared_error(means, y_pred_robust)
coeffs_robust = results_robust.params

print(f"Robust Regression: R² = {r2_robust:.4f}, MSE = {mse_robust:.4f}")
print(f"Коэффициенты: a = {coeffs_robust[1]:.4f}, b = {coeffs_robust[0]:.4f}")
```

    Robust Regression: R² = 0.0774, MSE = 3282.1459
    Коэффициенты: a = 14.3576, b = 0.5972


## Polyfit (полиномиальная регрессия с n = 1)


```python
coeffs_poly = np.polyfit(x, means, 1)
y_pred_poly = np.polyval(coeffs_poly, x)

r2_poly = r2_score(means, y_pred_poly)
mse_poly = mean_squared_error(means, y_pred_poly)

print(f"Polyfit: R² = {r2_poly:.4f}, MSE = {mse_poly:.4f}")
print(f"Коэффициенты: a = {coeffs_poly[0]:.4f}, b = {coeffs_poly[1]:.4f}")
```

    Polyfit: R² = 0.0776, MSE = 3281.5872
    Коэффициенты: a = 14.0425, b = -0.0506


## Ridge Regression (ридж-регрессия)


```python
# Ридж-регрессия
model_ridge = Ridge(alpha=1.0)
model_ridge.fit(X, means)
y_pred_ridge = model_ridge.predict(X)

# Метрики
r2_ridge = r2_score(means, y_pred_ridge)
mse_ridge = mean_squared_error(means, y_pred_ridge)
coeffs_ridge = model_ridge.coef_[0], model_ridge.intercept_

print(f"Ridge Regression: R² = {r2_ridge:.4f}, MSE = {mse_ridge:.4f}")
print(f"Коэффициенты: a = {coeffs_ridge[0]:.4f}, b = {coeffs_ridge[1]:.4f}")
```

    Ridge Regression: R² = 0.0776, MSE = 3281.6682
    Коэффициенты: a = 13.8020, b = -0.0506


## Визуализация результатов линейной аппроксимации


```python
plt.figure(figsize=(12, 8))

for i in range(10):
    plt.scatter(
        x,
        y[:, i],
        color="gray",
        alpha=0.3,
        s=30,
        label="Исходные точки" if i == 0 else "",
    )
plt.scatter(x, means, color="blue", marker="s", s=80, label="Средние значения")

# Отображение аппроксимирующих прямых
plt.plot(x, y_pred_linear, "r-", linewidth=2.5, label=f"Linear (R²={r2_linear:.3f})")
plt.plot(x, y_pred_robust, "g--", linewidth=2.5, label=f"Robust (R²={r2_robust:.3f})")
plt.plot(x, y_pred_poly, "b-.", linewidth=2.5, label=f"Polyfit (R²={r2_poly:.3f})")
plt.plot(x, y_pred_ridge, "m:", linewidth=2.5, label=f"Ridge (R²={r2_ridge:.3f})")

plt.grid(True, alpha=0.3)
plt.xlabel("x", fontsize=12)
plt.ylabel("y", fontsize=12)
plt.title("Сравнение методов линейной регрессии", fontsize=14)
plt.legend(loc="best", fontsize=10)
plt.tight_layout()
```


    
![png](2b_files/2b_31_0.png)
    


# Полиномиальная аппроксимация


```python
degrees = [1, 2, 3, 4, 5, 10, 20, 30, 40]
results_poly = []
for deg in degrees:
    coeffs = np.polyfit(x, means, deg)
    y_pred = np.polyval(coeffs, x)
    r2 = r2_score(means, y_pred)
    mse = mean_squared_error(means, y_pred)
    results_poly.append((deg, r2, mse, coeffs, y_pred))

    print(f"Степень {deg}: R² = {r2:.4f}, MSE = {mse:.4f}")
```

    Степень 1: R² = 0.0776, MSE = 3281.5872
    Степень 2: R² = 0.0787, MSE = 3277.8219
    Степень 3: R² = 0.2870, MSE = 2536.5996
    Степень 4: R² = 0.2870, MSE = 2536.5112
    Степень 5: R² = 0.8906, MSE = 389.1743
    Степень 10: R² = 0.9956, MSE = 15.7857
    Степень 20: R² = 0.9972, MSE = 9.7958
    Степень 30: R² = 0.9987, MSE = 4.5696
    Степень 40: R² = 0.9994, MSE = 2.1626


## Визуализация полиномиальных аппроксимаций


```python
plt.figure(figsize=(14, 10))
for i, (deg, r2, mse, coeffs, y_pred) in enumerate(results_poly):
    if deg in [1, 3, 5, 10, 40]:
        plt.plot(x, y_pred, linewidth=2, label=f"Степень {deg} (R²={r2:.3f})")

# Исходные данные
for i in range(10):
    plt.scatter(x, y[:, i], color="gray", alpha=0.3, s=30)
plt.scatter(x, means, color="blue", s=80, label="Средние значения")

plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Полиномиальные аппроксимации разных степеней")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fd5b3168550>




    
![png](2b_files/2b_35_1.png)
    


# Кусочная полиномиальная аппроксимация


```python
# Линейная интерполяция
f_linear = interp1d(x, means, kind="linear")
x_fine = np.linspace(min(X), max(x), 500)
y_linear = f_linear(x_fine)

# Кубическая интерполяция
f_cubic = interp1d(x, means, kind="cubic")
y_cubic = f_cubic(x_fine)

# PCHIP (полиномы Эрмита)
f_pchip = PchipInterpolator(x, means)
y_pchip = f_pchip(x_fine)

# Сплайны
spline = CubicSpline(x, means)
y_spline = spline(x_fine)
```

## Визуализация


```python
plt.figure(figsize=(12, 8))

# Отображение исходных данных
for i in range(10):
    plt.scatter(x, y[:, i], color="gray", alpha=0.3, s=30)
plt.scatter(x, means, color="blue", s=80, label="Средние значения")

# Отображение различных методов интерполяции
plt.plot(x_fine, y_linear, "r-", linewidth=2, label="Линейная интерполяция")
plt.plot(x_fine, y_cubic, "g--", linewidth=2, label="Кубическая интерполяция")
plt.plot(x_fine, y_pchip, "b-.", linewidth=2, label="PCHIP")
plt.plot(x_fine, y_spline, "m:", linewidth=2, label="Cubic Spline")

plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Кусочная полиномиальная аппроксимация")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fd5b2ffb110>




    
![png](2b_files/2b_39_1.png)
    


# Нелинейная аппроксимация


```python
def nonlinear_func(x, a0, a1, a2, beta):
    """y(x) = (sin(x) + β)(a2*x^2 + a1*x + a0)"""
    return (np.sin(x) + beta) * (a2 * x**2 + a1 * x + a0)


# Начальные приближения параметров
initial_guess = [1, 1, 1, 1]

# Нелинейная регрессия
params, covariance = curve_fit(nonlinear_func, x, means, p0=initial_guess)
y_pred_nonlinear = nonlinear_func(x, *params)

# Метрики
r2_nonlinear = r2_score(means, y_pred_nonlinear)
mse_nonlinear = mean_squared_error(means, y_pred_nonlinear)

print(f"Нелинейная регрессия: R² = {r2_nonlinear:.4f}, MSE = {mse_nonlinear:.4f}")
print(
    f"Параметры: a0={params[0]:.4f}, a1={params[1]:.4f}, a2={params[2]:.4f}, beta={params[3]:.4f}"
)
```

    Нелинейная регрессия: R² = 0.4643, MSE = 1905.8547
    Параметры: a0=-80.3604, a1=0.3627, a2=45.8674, beta=0.0292


## Визуализация


```python
plt.figure(figsize=(10, 6))

# Отображение исходных данных
for i in range(10):
    plt.scatter(x, y[:, i], color="gray", alpha=0.3, s=30)
plt.scatter(x, means, color="blue", s=80, label="Средние значения")

# Отображение нелинейной аппроксимации
plt.plot(
    x,
    y_pred_nonlinear,
    "r-",
    linewidth=2.5,
    label=f"Нелинейная аппроксимация (R²={r2_nonlinear:.3f})",
)

plt.grid(True, alpha=0.3)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Нелинейная аппроксимация")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7fd5b30bd450>




    
![png](2b_files/2b_43_1.png)
    


# Выводы


```python
# Сбор результатов в таблицу
results = []

# Линейные методы
results.append(
    (
        "Linear Regression",
        r2_linear,
        mse_linear,
        "Низкая",
        "Средняя",
        "Чувствителен к выбросам",
    )
)
results.append(
    (
        "Robust Regression",
        r2_robust,
        mse_robust,
        "Средняя",
        "Высокая",
        "Устойчив к выбросам",
    )
)
results.append(
    (
        "Polyfit (n=1)",
        r2_poly,
        mse_poly,
        "Низкая",
        "Средняя",
        "Аналогичен линейной регрессии",
    )
)
results.append(
    (
        "Ridge Regression",
        r2_ridge,
        mse_ridge,
        "Низкая",
        "Средняя",
        "Уменьшает переобучение",
    )
)

# Полиномиальные методы (выберем оптимальную степень)
optimal_deg = 5  # Например, степень 5 показала хорошие результаты
r2_optimal = next(r2 for deg, r2, mse, _, _ in results_poly if deg == optimal_deg)
mse_optimal = next(mse for deg, r2, mse, _, _ in results_poly if deg == optimal_deg)
results.append(
    (
        "Полиномиальная регрессия (оптимальная степень)",
        r2_optimal,
        mse_optimal,
        "Высокая",
        "Средняя",
        "Требует выбора степени",
    )
)

# Кусочная аппроксимация
r2_linear_interp = r2_score(means, f_linear(x))
mse_linear_interp = mean_squared_error(means, f_linear(x))
results.append(
    (
        "Линейная интерполяция",
        r2_linear_interp,
        mse_linear_interp,
        "Средняя",
        "Высокая",
        "Простота вычислений",
    )
)

r2_spline = r2_score(means, spline(x))
mse_spline = mean_squared_error(means, spline(x))
results.append(
    (
        "Cubic Spline",
        r2_spline,
        mse_spline,
        "Высокая",
        "Высокая",
        "Гладкость аппроксимации",
    )
)

# Нелинейная аппроксимация
results.append(
    (
        "Нелинейная регрессия",
        r2_nonlinear,
        mse_nonlinear,
        "Высокая",
        "Низкая",
        "Сложность выбора модели",
    )
)

# Создаем таблицу
df_results = pd.DataFrame(
    results,
    columns=[
        "Метод",
        "R²",
        "MSE",
        "Качество аппроксимации",
        "Вычислительная сложность",
        "Особенности",
    ],
)

# Сортируем по R² в порядке убывания
df_results = df_results.sort_values(by="R²", ascending=False).reset_index(drop=True)
df_results
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Метод</th>
      <th>R²</th>
      <th>MSE</th>
      <th>Качество аппроксимации</th>
      <th>Вычислительная сложность</th>
      <th>Особенности</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Cubic Spline</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>Высокая</td>
      <td>Высокая</td>
      <td>Гладкость аппроксимации</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Линейная интерполяция</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>Средняя</td>
      <td>Высокая</td>
      <td>Простота вычислений</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Полиномиальная регрессия (оптимальная степень)</td>
      <td>0.890609</td>
      <td>389.174295</td>
      <td>Высокая</td>
      <td>Средняя</td>
      <td>Требует выбора степени</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Нелинейная регрессия</td>
      <td>0.464295</td>
      <td>1905.854726</td>
      <td>Высокая</td>
      <td>Низкая</td>
      <td>Сложность выбора модели</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Polyfit (n=1)</td>
      <td>0.077598</td>
      <td>3281.587239</td>
      <td>Низкая</td>
      <td>Средняя</td>
      <td>Аналогичен линейной регрессии</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Linear Regression</td>
      <td>0.077598</td>
      <td>3281.587239</td>
      <td>Низкая</td>
      <td>Средняя</td>
      <td>Чувствителен к выбросам</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Ridge Regression</td>
      <td>0.077576</td>
      <td>3281.668184</td>
      <td>Низкая</td>
      <td>Средняя</td>
      <td>Уменьшает переобучение</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Robust Regression</td>
      <td>0.077441</td>
      <td>3282.145946</td>
      <td>Средняя</td>
      <td>Высокая</td>
      <td>Устойчив к выбросам</td>
    </tr>
  </tbody>
</table>
</div>



Анализируя результаты аппроксимации данных различными методами, можно сделать следующие выводы. Методы кусочной аппроксимации (Cubic Spline и линейная интерполяция) демонстрируют идеальную точность (R²=1.0, MSE=0), так как они по определению точно проходят через все заданные точки. Однако это не означает их превосходства для прогнозирования или обобщения данных, поскольку они склонны к переобучению и не сглаживают случайные флуктуации в измерениях. Полиномиальная регрессия с оптимально подобранной степенью показала наилучший баланс между точностью (R²=0.8906) и обобщающей способностью среди параметрических моделей, что делает ее предпочтительной для описания общей тенденции в данных. Нелинейная регрессия с использованием гармонических функций показала умеренные результаты (R²=0.4643), что может указывать на несоответствие выбранной модели реальной зависимости или недостаточную сложность модели для описания имеющихся данных. Все линейные методы (Linear Regression, Polyfit, Ridge Regression и Robust Regression) продемонстрировали крайне низкую объясняющую способность (R²≈0.077), что свидетельствует о явной нелинейности исследуемой зависимости, которую линейные модели не могут адекватно описать. Вычислительная сложность методов варьируется от низкой (нелинейная регрессия) до высокой (Cubic Spline, Robust Regression), что необходимо учитывать при работе с большими объемами данных. При выборе метода аппроксимации следует руководствоваться целями анализа: для интерполяции между известными точками предпочтительны сплайны, для построения обобщающей модели с балансом точности и простоты — полиномиальная регрессия с оптимальной степенью, а при наличии выбросов в данных — робастные методы, несмотря на их высокую вычислительную сложность. Важно отметить, что идеальная точность интерполяционных методов может быть обманчива для зашумленных данных, так как они аппроксимируют не только полезный сигнал, но и случайные погрешности измерений.
